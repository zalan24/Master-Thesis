Get a triangle on screen:
    ✔ Rework command lanes @done (1/19/2021, 11:12:55 AM)
        ✔ Command lane info contains queue infos (name, what's required, what's preferred, priority, etc.) @done (1/19/2021, 10:42:42 AM)
        ✔ When using the same queue, use the highest priority @done (1/19/2021, 10:42:45 AM)
        ✔ Try to use different queue families in a command lane @done (1/19/2021, 10:45:02 AM)
        ✔ Command lane tries to get these queues separately if possible @done (1/19/2021, 10:45:21 AM)
        ✔ Command lane will have a set of queues assigned to the names, which don't have to be unique (transfer and render queue can be the same) @done (1/19/2021, 10:45:28 AM)
        ✔ Command lanes used in the engine: @done (1/19/2021, 10:45:36 AM)
            ✔ Main @done (1/19/2021, 10:45:36 AM)
                ✔ Render (graphics queue) @done (1/19/2021, 10:45:35 AM)
                ✔ Compute (compute queue) @done (1/19/2021, 10:45:35 AM)
                ✔ DtoH (transfer queue) @done (1/19/2021, 10:45:34 AM)
                ✔ HtoD (transfer queue) @done (1/19/2021, 10:45:33 AM)
            ✔ Input @done (1/19/2021, 10:45:33 AM)
                ✔ HtoD (transfer queue) @done (1/19/2021, 10:45:32 AM)
    ✔ Modify queue manager to work with command lanes @done (1/19/2021, 11:12:08 PM)
        ✔ Have a mapping of CommandLane -> QueueName -> Queue @done (1/19/2021, 11:12:08 AM)
        ✔ Acquired queues based on this @done (1/19/2021, 11:12:08 AM)
        ✔ Remove hash based groupings (maybe some other stuff as well) @done (1/19/2021, 11:12:07 PM)
    ☐ Command buffer pool
        ✔ Initialize in engine @done (1/20/2021, 12:25:34 AM)
        ☐ CommandBuffer circulator:
            ✔ Only resetable one-time-submit command buffer are supported @done (1/22/2021, 9:25:39 PM)
            ✔ Is assigned to a family @done (1/22/2021, 9:25:43 PM)
            ✔ Have a Command pool @done (1/22/2021, 9:25:43 PM)
            ✔ The idea is that, upon cmd buffer request @done (1/22/2021, 9:25:46 PM)
                ✔ If there is an available buffer, return it @done (1/22/2021, 9:25:46 PM)
                    ✔ Prefer queues with lower index @done (1/22/2021, 9:25:47 PM)
                ✔ If not, create more command buffers @done (1/22/2021, 9:25:51 PM)
                ☐ If utilization is low for a long time, decrease the size by releasing the last X command buffers
            ✔ Upon recording the buffer, it should be sent to the execution thread with the circulator ptr @done (2/28/2021, 3:48:51 PM)
            ✔ Upon submission, give the pending buffer back to the circulator @done (2/28/2021, 3:48:53 PM)
            ✔ Dtor/close asserts/waits if there are some pending queues (that might be a problem when in a catch block...) @done (2/28/2021, 3:48:57 PM)
        ✔ Have a CommandBuffer circulator for each queue family @done (1/22/2021, 9:26:05 PM)
    ✔ Execution queue @done (2/28/2021, 3:49:10 PM)
        ✔ Create a struct which can store: (in union?) @done (1/27/2021, 10:20:31 PM)
            ✔ CommandBuffer handle @done (1/27/2021, 10:20:31 PM)
            ✔ std::function @done (1/27/2021, 10:20:32 PM)
            ✔ messages? @done (1/27/2021, 10:20:32 PM)
        ✔ Create a concurrent dequeue for this struct between recording and execution @done (1/27/2021, 10:20:34 PM)
        ✔ Execution is reading this pipe for instructions @done (1/27/2021, 10:20:36 PM)
        ✔ Execute command buffers @done (2/28/2021, 3:49:09 PM)
    ☐ Create a frame memory allocator
        ☐ It needs to support allocation on recording thread and release in the execution thread (multiple frame memory objects?)
        ☐ Use a pool of frame mems? acquire in simulation thread, release in execution thread
    ✔ Rework frame graph @done (2/28/2021, 3:49:29 PM)
    ✔ Implement barriers @done (1/27/2021, 11:38:06 AM)
    ✔ Implement timeline semaphores @done (2/28/2021, 3:49:41 PM)
    ✔ Implement events @done (1/26/2021, 11:30:31 AM)
    ☐ Implement graphical pipeline
    ☐ Implement render pass
    ☐ Command for reset command buffer
    ☐ Add a new thread for callbacks
        ☐ Pair of fences or events + std::function can be added to it
        ☐ Wait for fence/event execute function
    ☐ Shader manager
    ☐ Finish descriptors
    ✔ Add an event pool @done (3/2/2021, 9:34:57 AM)
        ✔ Use garbage system for recycling @done (3/2/2021, 9:34:57 AM)
    ☐ Resource tracking
        ☐ Implement several levels of sync precision
            ☐ Auto sync / just validate (auto sync is only for debug)
                ☐ Validate can assert or just warn
            ☐ Default: sync only as much as necessary
            ☐ Add a pipeline barrier to the end of the command buffer to make everything available
            ☐ Make everything available immediately
            ☐ Use general memory barriers instead of sub-resource based barriers
            ☐ More sync: remove sync optimizations
            ☐ Debug sync: always sync everything (all pipeline, all access mask, always)
            ☐ Do extra tracking and try to detect unnecessary syncs (export this data)
        ☐ Create a resource tracker object on the driver level
            ✔ Track any resource used in the commands @done (3/4/2021, 10:13:23 AM)
            ☐ Use a greedy algorithm for both availability and visibility and place pipeline barriers in cmd calls
            ☐ Add an optional arg to the cmd calls for event ptr
                ☐ If an event ptr is given, the cmd call should record a set event call
                ☐ This set event call should depend on the resources written in this call
                ☐ Event ptr should be registered in the tracking history
                ☐ Later calls can automatically wait on this event instead of using pipeline barriers
                ☐ Sync levels should apply to these events too
                ☐ API could make suggestions of event usage in debug mode???
                    ☐ If the pipeline barrier waits for extra pipeline stages, suggest event???
                    ☐ If event was not used, suggest to remove it???
            ☐ Handle semaphores (semaphores could bind with an other/self resource tracker)
        ☐ Inside a framegraph node (inter command buffer sync)
            ☐ Track any resources
            ☐ Framegraph nodes with gpu work should have their own resource tracker (for every queue)
                ☐ Keep only unregistered resources when ending the node
                ☐ Unregistered resources should keep tracking information for the next frame
                ☐ Only release acquired events (used in cmd calls), after they are released from tracking history
            ☐ Track resource usage in the command buffer recorder
                ☐ Resource
                ☐ Sub-resource range
                ☐ Pipeline stage mask
                ☐ Access mask
                ☐ Write/Read flags
            ☐ Add a memory barrier of the proper type (based on resource) to writing operations from history
            ☐ For write operations, also add barriers for any reading operations in history
            ☐ Ignore history entries, which are implicitly waited upon
            ☐ Resource trackers should be able to handle unregistered read only resources
                ☐ Tracker needs to see if the resource has been made visible to the proper shader stages
                ☐ If not, make it visible, but only once
                ☐ If the resource is visible by default, no sync should ever happen
        ☐ Track resources globally
            ☐ Created resource ids in the framegraph
                ☐ Resources are resolved in every frame (first usage of the res id)
                    ☐ Multiple physical resources can be bound to a res id (eg. images from gbuffer)
                ☐ Flag for last write (after which no sync is needed for read operations)???
                    ☐ Command buffer recorder should call a command to mark the last write
                    ☐ Intended to be used with Garbage system
                ☐ Flag for different resource every frame
                    ☐ No need to sync inter frame
                    ☐ This basically means the signalling of sync objects intended for the next frame
                ☐ Flag for global sub-resource range
                    ☐ When set, the sub-resource range is also recorded at resource resolution
                    ☐ Finer sync with events
                    ☐ Extra validation is needed
                ☐ No auto sync flag
                ☐ Special semaphore supplier
                    ☐ Instead of using the auto sync system, use a custom functor/something to know when and how to sync current resource usage
                    ☐ Intended for resources that are written only once (textures, meshes) and other custom resources
            ☐ Register the resource ids in the queue -> ... and ... -> queue dependencies
                ☐ Res id
                ☐ Read/write mask
            ☐ Validate registered resource usage
            ☐ Have a method in command buffer recorder for supplying a registered resource
                ☐ supplyRegisteredResource(resId, resource, sub_resource_range, pipelineStageMask, accessMask)
                ☐ signal semaphores and events based on dependencies in framegraph
                ☐ The exact event and semaphore should be known in the waiter commands (they might be recorded earlier)
            ☐ Have a method for demanding a registered resource
                ☐ demandRegisteredResource(resId, resource, sub_resource_range, pipelineStageMask, accessMask)
                ☐ This command buffer should wait on the proper semaphores and events
                ☐ Signal events and semaphores for writing nodes
            ☐ Maybe merge these two and add a mask for reading / writing???
            ☐ In case of a recorded usage of the resource doesn't conflict, there is no need to wait
            ☐ Once a resource is resolved by the supplier node, no sync is needed to different resource values (other image was used in the last frame)
            ☐ When waiting for a registered resource by event, just add it to the local tracker and it should do the work
            ☐ If there are no depending write operations for a resource, reads may not need to signal anything???
                ☐ Later frame writes can wait on frame finish instead???
            ☐ What about missing demand or supply operations???
        ☐ Validate resource usage
            ☐ Any resource
            ☐ Only debug mode
            ☐ Track resources globally in the framegraph (same way as in command buffer recorder)
            ☐ Use a mutex for recording usage history
            ☐ Find conflicting usages
            ☐ Allowed conflicting usage:
                ☐ Inside same node and same frame (need to register nodeId and frameId)
                ☐ Usage in a node/frame which has an appropriate dependency and the resource is registered
                    ☐ Need to check dependency offset and place of usage (cpu/queue)
        ☐ Validate synchronization using the link in sources

Cleanup & other:
    ☐ Remove useless common driver stuff (drv/common)
    ✔ Use abstract driver class instead of function pointer table @done (1/26/2021, 11:31:13 AM)
    ✔ Maybe use a single large stack memory (thread local) instead of local ones: LOCAL_MEMORY_POOL_DEFAULT(pool)? @done (2/28/2021, 3:50:11 PM)


Questions:
    ✔ Query 16 queue and use 1 vs Query 1 queue and use 1 @done (1/18/2021, 12:28:09 PM)
        -> allocation cost
        -> can have extra perf cost as well. Don't do it
    ✔ Synchronize between two queue after every command (semaphores), but no actual waiting happens @done (1/18/2021, 12:28:30 PM)
        -> semaphore overhead is negligible
    ✔ 16 draw calls on 16 queues vs 16 draw calls on 1 queue (sync for depth test and sync for color write) @done (1/18/2021, 12:29:45 PM)
        -> no point in separation, card already runs in parallel. Set up render passes properly though
        -> cs and transfer can be separate
    ✔ How vk submit waits on semaphores, where stages are provided (command buffers might contain a lot of pipelines) @done (1/18/2021, 12:37:58 PM)
        -> all commands wait until that stage on the semaphore
    ✔ 100 commands on same buffer vs on separate buffers @done (1/18/2021, 12:29:18 PM)
        -> consumes memory / more api calls. Use a single buffer if possible
    ✔ Can I record commands to two separate command buffers, which share a command pool concurrently? @done (1/26/2021, 11:31:18 AM)
        -> No
    ☐ Transient command buffer vs non-transient (except for parallel execution)

    https://stackoverflow.com/questions/37575012/should-i-try-to-use-as-many-queues-as-possible

    ☐ Read: https://mynameismjp.wordpress.com/2018/06/17/breaking-down-barriers-part-3-multiple-command-processors/
    ☐ Read: https://community.arm.com/developer/tools-software/graphics/b/blog/posts/vulkan-mobile-best-practices-and-management